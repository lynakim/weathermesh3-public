from data import *
import pickle
import numpy as np
import torch
torch.manual_seed(0)
from hres_utils import *

from model import HresModel

from distributed_shampoo.distributed_shampoo import DistributedShampoo
from distributed_shampoo.utils.shampoo_utils import GraftingType

interps, idxs = pickle.load(open("/fast/ignored/hres/interps.pickle", "rb"))
station_list = pickle.load(open("/fast/ignored/hres/station_list.pickle", "rb"))
statics = pickle.load(open("/fast/ignored/hres/statics.pickle", "rb"))

class Trainer:
    def __init__(self):
        self.B = 512
        dataset = HresDataset(batch_size=self.B)
        self.loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)
        print("uh oh", len(dataset))

        self.model = HresModel()
        print("number of parameters", sum([x.numel() for x in self.model.parameters()])/1e6, "M")
        self.device = torch.device("cuda")
        self.model = self.model.to(self.device)
        if HALF:
            self.model = self.model.half()
        self.interps = torch.tensor(interps).to(self.device)
        self.idxs = torch.tensor(idxs).to(self.device)
        self.statics = {x: torch.tensor(y).to(self.device) for x, y in statics.items()}

        self.scaler = torch.cuda.amp.GradScaler()

        self.lr = 1e-3

        self.config = SimpleNamespace()
        self.config.lr_sched = SimpleNamespace()
        self.config.lr_sched.lr = 2e-4
        self.config.lr_sched.warmup_end_step = 2_500
        self.config.lr_sched.step_offset = 0
        self.config.lr_sched.div_factor = 4
        self.config.lr_sched.cosine_period = 800_000
        self.config.lr_sched.cosine_en = 1
        self.config.lr_sched.cosine_bottom = None


    def setup_optimizer(self):
        self.optimizer = DistributedShampoo(
                    [x for x in self.model.parameters() if x.requires_grad],
                    lr=1e-2,
                    betas=self.conf.adam.betas,
                    epsilon=1e-12,
                    weight_decay=1e-6,
                    max_preconditioner_dim=2048,#8192,
                    precondition_frequency=1, # you probably want to start with this at like, 75!
                    start_preconditioning_step=1,
                    use_decoupled_weight_decay=True,
                    grafting_type=GraftingType.ADAM,
                    grafting_epsilon=1e-08,
                    grafting_beta2=0.999,
                    #num_trainers_per_group=4,
                )

    def setup(self):
        self.setup_optimizer()
    
    def train(self):
        last_loop = None
        for i, sample in enumerate(self.loader):
            if last_loop is not None:
                print("loading time", time.time()-last_loop)
            era5, data, sample_stations, sample_valid = sample
            assert era5["sfc"].shape[0] == 1
            st0 = time.time()

            data = data.to(self.device)
            #print(sample_stations, sample_stations.shape)
            #sample_era5 = era5[0, ]
            #print(sample_stations)
            #print(list(set(sample_stations[0])))
            #print("uniq", len(set([int(x) for x in sample_stations[0]])))
            interp = self.interps[sample_stations]
            idx = self.idxs[sample_stations]
            """
            pr = era5["pr"].to(self.device)
            pr_sample = pr[0, idx[..., 0], idx[..., 1], :, :]
            pr_sample = torch.sum(pr_sample * interp[:,:,:,:,None,None], axis=3)[0]
            """

            sfc = era5["sfc"].to(self.device)
            sfc_sample = sfc[0, idx[..., 0], idx[..., 1], :]
            sfc_sample = torch.sum(sfc_sample * interp[:,:,:,:,None], axis=3)[0]
            sera5 = self.statics["era5"][sample_stations][0]
            #print("uhh", sera5[:, 0, 0].max())
            sera5[:, :, 0] /= 1000. * 9.8
            sera5[:, :, 1] /= 7.
            sera5[:, :, 3] /= 20.
            sera5[:, :, 4] /= 20.
            sfc_sample = torch.cat((sfc_sample, sera5), axis=-1)

            #center_pr = pr_sample[:,0]
            center_sfc = sfc_sample[:,0]

            #pr_sample = pr_sample[:, 1:]
            sfc_sample = sfc_sample[:, 1:]
            sq = int(np.sqrt(sfc_sample.shape[1]))
            sfc_sample = sfc_sample.permute(0, 2, 1).view(-1, 9, sq, sq)

            static_keys = sorted(statics.keys())
            static_keys.remove("era5")
            static = {x: self.statics[x][sample_stations][0] for x in static_keys}
            center = {x: static[x][:,0,0] for x in static}
            for x in static_keys:
                sq = int(np.sqrt(static[x].shape[1]-1))
                static[x] = static[x][:,1:].view(-1, sq, sq, 3)
                if x.startswith("mn"):
                    #print("hullo", static[x][:,:,:,0].mean(), center[x][:,None,None].mean(), static[x].shape, center[x].shape)
                    static[x][:,:,:,0] = (static[x][:,:,:,0] - center[x][:,None,None])*(1./150)
                    static[x][:,:,:,1:] /= 20.
                    center[x] = center[x]*(1./1000)
                static[x] = static[x].permute(0, 3, 1, 2)
            inp = {}
            #inp["pr"] = pr_sample
            inp["sfc"] = sfc_sample
            for k in static:
                inp[k] = static[k]
            inp["center"] = torch.stack([center[x] for x in static_keys], dim=1)
            inp["center"] = torch.cat([inp["center"], center_sfc], dim=1)
            inp = {x: y.half() for x, y in inp.items()}
            #inp["sfc_center"] = center_sfc

            #inp = {x: y[:1024] for x, y in inp.items()}

            if 0:
                for x in inp:
                    y = inp[x]
                    print(x, inp[x].shape, inp[x].dtype, inp[x].device, torch.mean(y), torch.std(y), torch.min(y), torch.max(y))
                    if x.startswith("center"):
                        print("ohp", torch.std(y, axis=0))
                    if x.startswith("mn"):
                        #print("0", y[0, 0, :, :])
                        #print("1", y[0, 1, :, :])
                        #print("2", y[0, 2, :, :])
                        print("ohp", torch.std(y, axis=(0,2,3)))
                exit()

            torch.cuda.synchronize()
            print("processing shit", time.time()-st0)
            
 

            y = self.model(inp)
            bw = time.time()
            loss = torch.abs(y - data[0]).mean()
            loss.backward()
            torch.cuda.synchronize()
            dt = time.time()-bw
            print("total", dt, "abs", time.time()-st0)

            last_loop = time.time()
            

wt = Trainer()
wt.train()